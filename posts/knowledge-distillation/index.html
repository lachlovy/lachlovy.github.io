<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>知识蒸馏简述 | Lachlovy's Blog</title>
<meta name=keywords content><meta name=description content="本文基于 Hinton 在 2015 年的论文《Distilling the Knowledge in a Neural Network》的研究并结合后续知识蒸馏技术研究整理而成。"><meta name=author content><link rel=canonical href=https://lachlovy.github.io/posts/knowledge-distillation/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://lachlovy.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lachlovy.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lachlovy.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://lachlovy.github.io/apple-touch-icon.png><link rel=mask-icon href=https://lachlovy.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://lachlovy.github.io/posts/knowledge-distillation/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta property="og:url" content="https://lachlovy.github.io/posts/knowledge-distillation/"><meta property="og:site_name" content="Lachlovy's Blog"><meta property="og:title" content="知识蒸馏简述"><meta property="og:description" content="本文基于 Hinton 在 2015 年的论文《Distilling the Knowledge in a Neural Network》的研究并结合后续知识蒸馏技术研究整理而成。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-07T16:20:22+08:00"><meta property="article:modified_time" content="2025-03-07T16:20:22+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="知识蒸馏简述"><meta name=twitter:description content="本文基于 Hinton 在 2015 年的论文《Distilling the Knowledge in a Neural Network》的研究并结合后续知识蒸馏技术研究整理而成。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://lachlovy.github.io/posts/"},{"@type":"ListItem","position":2,"name":"知识蒸馏简述","item":"https://lachlovy.github.io/posts/knowledge-distillation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"知识蒸馏简述","name":"知识蒸馏简述","description":"本文基于 Hinton 在 2015 年的论文《Distilling the Knowledge in a Neural Network》的研究并结合后续知识蒸馏技术研究整理而成。","keywords":[],"articleBody":"知识蒸馏简述 本文还在逐步建设中。\n本文基于 Hinton 在 2015 年的论文《Distilling the Knowledge in a Neural Network》的研究并结合后续知识蒸馏技术研究整理而成。\n1. 背景与动机 机器学习过程中，训练和部署阶段的需求不一样。在训练阶段，会从大量数据中提取数据结构，此阶段可能会非常耗时且不需要实时操作。而部署阶段对于大量用户的使用情况，通常对实时性和延迟提出了更高的要求。训练多个模型并集成，或者训练一个非常大的模型都无法满足部署阶段的要求。因此 Hinton 及 Rich Caruana 提出的方法对模型进行“压缩”，将大模型知识转移到小模型上。\n2. 温度 $T$ 神经网络使用 Softmax 层来将模型输出转为一个概率分布。\n$$q_i = \\frac{exp(z_i/T)}{\\sum_jexp(z_i/T)}$$\n此处 $T$ 是一个可调节参数，当 $T=1$ 时，则是标准 softmax 函数，$\\sigma(\\vec{z}) = \\frac{exp(z_i)}{\\sum^K_{j=1}exp(z_j)}$\n随着温度 $T$ 当增加，指数运算之后的数值之间差值变小，导致概率分布更加平滑。\n例如，$T = 2$ 时，$q_i=\\frac{exp(z_i/2)}{\\sum_jexp(z_i/2)}$\n对于数据分布为 [0.3, 0.6, 0.1] 的数据，经过指数运算后得到 [1.2214, 1.8221, 1.1051]。\n经过温度 $T$ 调节之后为 [0.15, 0.3, 0.05]，经过指数运算得到 [1.1618, 1.3498, 1.0512]\n通过增加 $T$，概率分布更加平滑，最高概率与其他概率之间的差距变小。\n同理，减小温度 $T$ 会导致概率分布更加尖锐，最高概率的占比会增加，使得模型有更多“把握”选择更相信的选项。\n需要从概率分布中负标签学习更多的信息，应增大温度 $T$。 需要防止负标签噪音产生影响，应减小温度 $T$。 如论文中例子所说，宝马车被分类为垃圾车的概率很小，但是仍然比分类为萝卜的概率大得多。通过更大的温度设定可以放大这些非高概率项之间的关系。\n通过设置不同调整温度 $T$ 的值，在大语言模型生成 token 采样时，会有不同的表现。较小的温度会生成更加保守的结果，而更大的温度会生成更加多样性的结果，这对不同生成需求的任务会有不同的要求。\n3. 基础知识蒸馏方法 基础知识蒸馏方法利用一个教师网络和一个学生网络，通过两个训练阶段同时进行，让学生网络（较小参数量）能够实现学习到教师网络的知识。\n3.1 知识蒸馏 训练一个复杂的“教师”网络，可以是多个复杂模型的集成，也可以是一个巨大的网络。\n将教师网络产生的软标签（通过增加温度 $T$ 来“软化” softmax），来训练一个更简单的学生网络，使学生网络能够学习到教师网络中的\"暗知识\"（dark knowledge），包括类别之间的相似性信息。\n蒸馏的过程由一个联合损失函数决定的：$\\mathcal{L} = \\alpha Loss(soft) + \\beta Loss(hard)$。\n在推理时设置温度 $1$，按照正常预测流程进行。\n学生网络不仅能够学习正确的分类结果，也能学到教师网络对不同类别的置信度分布，从而在保持较好性能的同时大幅减小模型尺寸和推理成本。类别之间的相似性信息例如手写数字识别中，预测一个数字 2 的图片，在预测正确情况下数字 2 概率最大，但是数字 3 的概率也更靠近数字 2 的概率，说明这两个数字是更不容易被区分的。\n3.2 实验验证 MNIST 实验 在 MNIST 手写数字识别数据集（全部 60000 条数据）上使用不同规模的网络进行实验。\n两层 1200 个 ReLU 单元网络，达到 67 个测试错误数据。 两层 800 个 ReLU 单元网络，达到 146 个测试错误数据。在此模型仅仅添加温度为 20 的蒸馏过程后，达到了 74 个测试错误数据。 对于数据集中去掉所有包含数字 3 的图片的情况，数字 3 的出现对蒸馏模型来说是未见过的数据。尽管如此，蒸馏模型只在测试集中达到了 206 个测试错误，其中 133 个是在 1010 个数字 3 的测试集中。\n3.3 蒸馏有效性 为什么经过高温蒸馏后，模型的性能会上升？\n根据上一节 MNIST 实验我们可以看出，只对学生网络进行训练，测试错误数量依旧比较多，而经过高温蒸馏后，教师网络额外提供了一个概率分布，弥补了分类监督学习中信息不足的情况。例如，对于 MNIST 分类任务，每张图片有一个对应的标签，在数据集中通过标签表现出来数字 2 与数字 3 没有任何关系，即使它们在图形上是有相似部分的。MNIST 数据集提供了一个类似 one-hot 编码形式的数据进行训练，而软标签不仅提供了最大分类概率对应的标签，还提供了标签与标签之间的关系。\n因此，如果将 MNIST 的类别进行拓展，例如数字 1 与数字 7 分为一类（代表既像数字 1 也像数字 7 的类别），就能增加额外的信息，软标签达到的就是这样的效果。\n4. 训练方法 4.1 离线蒸馏 4.2 在线蒸馏 4.3 自我蒸馏 4.4 通用优化技术 4.4.1 动态调整温度参数 传统知识蒸馏方法存在的一个不足点在于：温度参数作为一个超参数，需要人工去调整找到一个性能好的参数，即使找到一个较好的温度参数，而在学生网络学习过程中，学生学习的阶段难度是在变化的。\n而文章 Curriculum Temperature for Knowledge Distillation 提出了一种由课程学习（Curriculum Learning）启发的动态调整温度的方法 Curriculum Temperature for Knowledge Distillation（CTKD）。课程学习通过组织学习任务的顺序，逐渐增加学习难度，从而提高模型的训练效果。\nCTKD 通过优化两点来达到动态调整温度参数：学生模型旨在最小化损失函数，而温度模块旨在最大化学生与与教师之间的蒸馏损失。\nCTKD 探索了两种动态调整温度的方法：\nGlobal-T：为所有实例预测一个温度值，计算成本低。 Instance-T：为每个实例单独预测一个温度，表示能力更强，但计算成本较高。 CTKD 可以无缝集成到现有的知识蒸馏框架中，来得到提升。\n5. 现代知识蒸馏技术 5.1 知识蒸馏分类 Response-based\nHinton 提出的基础知识蒸馏方法，根据模型的输出层结果来进行蒸馏。\nFeature-based\n教师网络中间层能捕捉到数据的特定特征，基于特征的知识蒸馏通过最小化教师模型和学生模型之间的特征激活来实现。\n例如有 Fitnet(Romero et al. 2015) 模型。\nRelation-based\n教师网络不仅能提供单个样本的信息，还能捕捉样本（样本对、样本三元组）之间的关系，更好地学习样本之间的关系。\nSelf-distillation\n不需要一个额外的教师网络，例如用更深层的网络来训练较浅层的网络。\n5.2 知识蒸馏算法 5.2.1 对抗性蒸馏 5.2.2 多教师蒸馏 5.2.3 跨模态蒸馏 5.2.4 其他算法 基于图的蒸馏 基于注意力的蒸馏 无数据蒸馏 量化蒸馏 终生蒸馏 更多的关于知识蒸馏方法可以查看综述论文：Knowledge Distillation: A Survey 了解更多。\n总结 知识蒸馏技术在模型部署阶段意义重大，一个训练好的大神经网络模型投入到用户使用时面对模型输出速度、延迟方面表现不如小模型，通过知识蒸馏技术，小模型能够达到大模型大部分的能力，并且保持较快的计算效率。面对越来越多的小型设备（手机、个人电脑），将最前沿的模型蒸馏后部署到小型设备上也成为了一个新的方向，能够让更多人体验到人工智能给我们带来的好处。\n参考文献 [1] Hinton, G., Vinyals, O., \u0026 Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\n[2] Li, Z., Li, X., Yang, L., Zhao, B., Song, R., Luo, L., … \u0026 Yang, J. (2023, June). Curriculum temperature for knowledge distillation. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 2, pp. 1504-1512).\n[3] Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., \u0026 Bengio, Y. (2014). Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550.\n[4] Gou, J., Yu, B., Maybank, S. J., \u0026 Tao, D. (2021). Knowledge distillation: A survey. International Journal of Computer Vision, 129(6), 1789-1819.\n","wordCount":"2680","inLanguage":"en","datePublished":"2025-03-07T16:20:22+08:00","dateModified":"2025-03-07T16:20:22+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://lachlovy.github.io/posts/knowledge-distillation/"},"publisher":{"@type":"Organization","name":"Lachlovy's Blog","logo":{"@type":"ImageObject","url":"https://lachlovy.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lachlovy.github.io/ accesskey=h title="Lachlovy's Blog (Alt + H)">Lachlovy's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://lachlovy.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">知识蒸馏简述</h1><div class=post-meta><span title='2025-03-07 16:20:22 +0800 +0800'>March 7, 2025</span>&nbsp;·&nbsp;6 min</div></header><div class=post-content><h1 id=知识蒸馏简述>知识蒸馏简述<a hidden class=anchor aria-hidden=true href=#知识蒸馏简述>#</a></h1><blockquote><p>本文还在逐步建设中。</p></blockquote><p>本文基于 Hinton 在 2015 年的论文《Distilling the Knowledge in a Neural Network》的研究并结合后续知识蒸馏技术研究整理而成。</p><h2 id=1-背景与动机>1. 背景与动机<a hidden class=anchor aria-hidden=true href=#1-背景与动机>#</a></h2><p>机器学习过程中，训练和部署阶段的需求不一样。在训练阶段，会从大量数据中提取数据结构，此阶段可能会非常耗时且不需要实时操作。而部署阶段对于大量用户的使用情况，通常对实时性和延迟提出了更高的要求。训练多个模型并集成，或者训练一个非常大的模型都无法满足部署阶段的要求。因此 Hinton 及 Rich Caruana 提出的方法对模型进行“压缩”，将大模型知识转移到小模型上。</p><h2 id=2-温度-t>2. 温度 $T$<a hidden class=anchor aria-hidden=true href=#2-温度-t>#</a></h2><p>神经网络使用 <code>Softmax</code> 层来将模型输出转为一个概率分布。</p><p>$$q_i = \frac{exp(z_i/T)}{\sum_jexp(z_i/T)}$$</p><p>此处 $T$ 是一个可调节参数，当 $T=1$ 时，则是标准 <code>softmax</code> 函数，$\sigma(\vec{z}) = \frac{exp(z_i)}{\sum^K_{j=1}exp(z_j)}$</p><p>随着温度 $T$ 当增加，指数运算之后的数值之间差值变小，导致概率分布更加平滑。</p><p>例如，$T = 2$ 时，$q_i=\frac{exp(z_i/2)}{\sum_jexp(z_i/2)}$</p><p>对于数据分布为 <code>[0.3, 0.6, 0.1]</code> 的数据，经过指数运算后得到 <code>[1.2214, 1.8221, 1.1051]</code>。</p><p>经过温度 $T$ 调节之后为 <code>[0.15, 0.3, 0.05]</code>，经过指数运算得到 <code>[1.1618, 1.3498, 1.0512]</code></p><p>通过增加 $T$，概率分布更加平滑，最高概率与其他概率之间的差距变小。</p><p>同理，减小温度 $T$ 会导致概率分布更加尖锐，最高概率的占比会增加，使得模型有更多“把握”选择更相信的选项。</p><ul><li>需要从概率分布中负标签学习更多的信息，应增大温度 $T$。</li><li>需要防止负标签噪音产生影响，应减小温度 $T$。</li></ul><p>如论文中例子所说，宝马车被分类为垃圾车的概率很小，但是仍然比分类为萝卜的概率大得多。通过更大的温度设定可以放大这些非高概率项之间的关系。</p><p>通过设置不同调整温度 $T$ 的值，在大语言模型生成 token 采样时，会有不同的表现。较小的温度会生成更加保守的结果，而更大的温度会生成更加多样性的结果，这对不同生成需求的任务会有不同的要求。</p><h2 id=3-基础知识蒸馏方法>3. 基础知识蒸馏方法<a hidden class=anchor aria-hidden=true href=#3-基础知识蒸馏方法>#</a></h2><p>基础知识蒸馏方法利用一个教师网络和一个学生网络，通过两个训练阶段同时进行，让学生网络（较小参数量）能够实现学习到教师网络的知识。</p><h3 id=31-知识蒸馏>3.1 知识蒸馏<a hidden class=anchor aria-hidden=true href=#31-知识蒸馏>#</a></h3><ol><li><p>训练一个复杂的“教师”网络，可以是多个复杂模型的集成，也可以是一个巨大的网络。</p></li><li><p>将教师网络产生的软标签（通过增加温度 $T$ 来“软化” <code>softmax</code>），来训练一个更简单的学生网络，使学生网络能够学习到教师网络中的"暗知识"（dark knowledge），包括类别之间的相似性信息。</p><p>蒸馏的过程由一个联合损失函数决定的：$\mathcal{L} = \alpha Loss(soft) + \beta Loss(hard)$。</p></li><li><p>在推理时设置温度 $1$，按照正常预测流程进行。</p></li></ol><p>学生网络不仅能够学习正确的分类结果，也能学到教师网络对不同类别的置信度分布，从而在保持较好性能的同时大幅减小模型尺寸和推理成本。类别之间的相似性信息例如手写数字识别中，预测一个数字 <code>2</code> 的图片，在预测正确情况下数字 <code>2</code> 概率最大，但是数字 <code>3</code> 的概率也更靠近数字 <code>2</code> 的概率，说明这两个数字是更不容易被区分的。</p><h3 id=32-实验验证>3.2 实验验证<a hidden class=anchor aria-hidden=true href=#32-实验验证>#</a></h3><ul><li>MNIST 实验</li></ul><p>在 MNIST 手写数字识别数据集（全部 60000 条数据）上使用不同规模的网络进行实验。</p><ol><li>两层 1200 个 <code>ReLU</code> 单元网络，达到 67 个测试错误数据。</li><li>两层 800 个 <code>ReLU</code> 单元网络，达到 146 个测试错误数据。在此模型仅仅添加温度为 20 的蒸馏过程后，达到了 74 个测试错误数据。</li></ol><p>对于数据集中去掉所有包含数字 3 的图片的情况，数字 3 的出现对蒸馏模型来说是未见过的数据。尽管如此，蒸馏模型只在测试集中达到了 206 个测试错误，其中 133 个是在 1010 个数字 3 的测试集中。</p><h3 id=33-蒸馏有效性>3.3 蒸馏有效性<a hidden class=anchor aria-hidden=true href=#33-蒸馏有效性>#</a></h3><blockquote><p>为什么经过高温蒸馏后，模型的性能会上升？</p></blockquote><p>根据上一节 MNIST 实验我们可以看出，只对学生网络进行训练，测试错误数量依旧比较多，而经过高温蒸馏后，<strong>教师网络额外提供了一个概率分布，弥补了分类监督学习中信息不足的情况</strong>。例如，对于 MNIST 分类任务，每张图片有一个对应的标签，在数据集中通过标签表现出来数字 <code>2</code> 与数字 <code>3</code> 没有任何关系，即使它们在图形上是有相似部分的。MNIST 数据集提供了一个类似 <code>one-hot</code> 编码形式的数据进行训练，而软标签不仅提供了最大分类概率对应的标签，还提供了标签与标签之间的关系。</p><p>因此，如果将 MNIST 的类别进行拓展，例如数字 <code>1</code> 与数字 <code>7</code> 分为一类（代表既像数字 <code>1</code> 也像数字 <code>7</code> 的类别），就能增加额外的信息，软标签达到的就是这样的效果。</p><h2 id=4-训练方法>4. 训练方法<a hidden class=anchor aria-hidden=true href=#4-训练方法>#</a></h2><p><img alt=知识蒸馏训练方法 loading=lazy src=https://img.imgdd.com/9928dfcf-d80c-4891-ae69-36b953700496.png></p><h3 id=41-离线蒸馏>4.1 离线蒸馏<a hidden class=anchor aria-hidden=true href=#41-离线蒸馏>#</a></h3><h3 id=42-在线蒸馏>4.2 在线蒸馏<a hidden class=anchor aria-hidden=true href=#42-在线蒸馏>#</a></h3><h3 id=43-自我蒸馏>4.3 自我蒸馏<a hidden class=anchor aria-hidden=true href=#43-自我蒸馏>#</a></h3><h3 id=44-通用优化技术>4.4 通用优化技术<a hidden class=anchor aria-hidden=true href=#44-通用优化技术>#</a></h3><h4 id=441-动态调整温度参数>4.4.1 动态调整温度参数<a hidden class=anchor aria-hidden=true href=#441-动态调整温度参数>#</a></h4><p>传统知识蒸馏方法存在的一个不足点在于：温度参数作为一个超参数，需要人工去调整找到一个性能好的参数，即使找到一个较好的温度参数，而在学生网络学习过程中，学生学习的阶段<em>难度</em>是在变化的。</p><p>而文章 <a href=https://ojs.aaai.org/index.php/AAAI/article/view/25236>Curriculum Temperature for Knowledge Distillation</a> 提出了一种由课程学习（Curriculum Learning）启发的动态调整温度的方法 Curriculum Temperature for Knowledge Distillation（CTKD）。课程学习通过组织学习任务的顺序，逐渐增加学习难度，从而提高模型的训练效果。</p><p>CTKD 通过优化两点来达到动态调整温度参数：学生模型旨在最小化损失函数，而温度模块旨在最大化学生与与教师之间的蒸馏损失。</p><p>CTKD 探索了两种动态调整温度的方法：</p><ul><li>Global-T：为所有实例预测一个温度值，计算成本低。</li><li>Instance-T：为每个实例单独预测一个温度，表示能力更强，但计算成本较高。</li></ul><p>CTKD 可以无缝集成到现有的知识蒸馏框架中，来得到提升。</p><h2 id=5-现代知识蒸馏技术>5. 现代知识蒸馏技术<a hidden class=anchor aria-hidden=true href=#5-现代知识蒸馏技术>#</a></h2><h3 id=51-知识蒸馏分类>5.1 知识蒸馏分类<a hidden class=anchor aria-hidden=true href=#51-知识蒸馏分类>#</a></h3><p><img alt=知识蒸馏分类 loading=lazy src=https://img.imgdd.com/32401b66-6c39-4fe2-add9-e0d6c6e6a672.png></p><ul><li><p>Response-based</p><p>Hinton 提出的基础知识蒸馏方法，根据模型的输出层结果来进行蒸馏。</p></li><li><p>Feature-based</p><p>教师网络中间层能捕捉到数据的特定特征，基于特征的知识蒸馏通过最小化教师模型和学生模型之间的特征激活来实现。</p><p>例如有 <a href=https://arxiv.org/abs/1412.6550>Fitnet(Romero et al. 2015)</a> 模型。</p></li><li><p>Relation-based</p><p>教师网络不仅能提供单个样本的信息，还能捕捉样本（样本对、样本三元组）之间的关系，更好地学习样本之间的关系。</p></li><li><p>Self-distillation</p><p>不需要一个额外的教师网络，例如用更深层的网络来训练较浅层的网络。</p></li></ul><h3 id=52-知识蒸馏算法>5.2 知识蒸馏算法<a hidden class=anchor aria-hidden=true href=#52-知识蒸馏算法>#</a></h3><h4 id=521-对抗性蒸馏>5.2.1 对抗性蒸馏<a hidden class=anchor aria-hidden=true href=#521-对抗性蒸馏>#</a></h4><h4 id=522-多教师蒸馏>5.2.2 多教师蒸馏<a hidden class=anchor aria-hidden=true href=#522-多教师蒸馏>#</a></h4><h4 id=523-跨模态蒸馏>5.2.3 跨模态蒸馏<a hidden class=anchor aria-hidden=true href=#523-跨模态蒸馏>#</a></h4><h4 id=524-其他算法>5.2.4 其他算法<a hidden class=anchor aria-hidden=true href=#524-其他算法>#</a></h4><ul><li>基于图的蒸馏</li><li>基于注意力的蒸馏</li><li>无数据蒸馏</li><li>量化蒸馏</li><li>终生蒸馏</li></ul><p>更多的关于知识蒸馏方法可以查看综述论文：<a href=https://link.springer.com/article/10.1007/s11263-021-01453-z>Knowledge Distillation: A Survey</a> 了解更多。</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>知识蒸馏技术在模型部署阶段意义重大，一个训练好的大神经网络模型投入到用户使用时面对模型输出速度、延迟方面表现不如小模型，通过知识蒸馏技术，小模型能够达到大模型大部分的能力，并且保持较快的计算效率。面对越来越多的小型设备（手机、个人电脑），将最前沿的模型蒸馏后部署到小型设备上也成为了一个新的方向，能够让更多人体验到人工智能给我们带来的好处。</p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><p>[1] Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.</p><p>[2] Li, Z., Li, X., Yang, L., Zhao, B., Song, R., Luo, L., &mldr; & Yang, J. (2023, June). Curriculum temperature for knowledge distillation. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 2, pp. 1504-1512).</p><p>[3] Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., & Bengio, Y. (2014). Fitnets: Hints for thin deep nets. <em>arXiv preprint arXiv:1412.6550</em>.</p><p>[4] Gou, J., Yu, B., Maybank, S. J., & Tao, D. (2021). Knowledge distillation: A survey. <em>International Journal of Computer Vision</em>, <em>129</em>(6), 1789-1819.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://lachlovy.github.io/posts/uv/><span class=title>« Prev</span><br><span>uv 入门指南</span>
</a><a class=next href=https://lachlovy.github.io/posts/welcome-to-hugo/><span class=title>Next »</span><br><span>Welcome to Hugo</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://lachlovy.github.io/>Lachlovy's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>